{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Original Format\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw as Drawer\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from ignite.handlers import FastaiLRFinder\n",
    "import torch\n",
    "\n",
    "# path to the satcen dataset\n",
    "satcen_path = pathlib.Path('../satcen_dataset/full').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full Satcen dataset\n",
    "satcen = os.listdir(satcen_path/'pictures')\n",
    "\n",
    "# load the labels (bounding boxes)\n",
    "labels_file = open(satcen_path/'labels.json')\n",
    "labels_json = json.load(labels_file)\n",
    "\n",
    "# define a helper function that returns the list of bounding boxes given a filename\n",
    "def get_bounding_boxes(filename):\n",
    "    return list(filter(lambda x: x['name'] == filename, labels_json))[0]['bounding_boxes']\n",
    "\n",
    "# convert the bounding boxes into binary labels 0/1\n",
    "y = np.array([])\n",
    "for img in satcen:\n",
    "\n",
    "    bounding_boxes = get_bounding_boxes(img)\n",
    "    y = np.append(y, 1 if len(bounding_boxes) > 0 else 0)\n",
    "\n",
    "# print info about entire dataset\n",
    "print(f'Total number of Satcen images: {len(satcen)}')\n",
    "print(f'Total number of labels (annotations): {len(labels_json)}')\n",
    "print(f'Number of positive observations: {np.sum(y==1)} ({100 * np.sum(y==1) / len(y):.2f}%)')\n",
    "print(f'Number of negative observations: {np.sum(y==0)} ({100 * np.sum(y==0) / len(y):.2f}%)\\n')\n",
    "\n",
    "# load the data\n",
    "train_images = os.listdir(satcen_path/'splits/train/images')\n",
    "valid_images = os.listdir(satcen_path/'splits/validation/images')\n",
    "test_images = os.listdir(satcen_path/'splits/test/images')\n",
    "\n",
    "# compute binary labels for each set\n",
    "y_train = np.array([y[satcen.index(img)] for img in train_images])\n",
    "y_valid = np.array([y[satcen.index(img)] for img in valid_images])\n",
    "y_test = np.array([y[satcen.index(img)] for img in test_images])\n",
    "\n",
    "# print stats about the splits\n",
    "print('Train dataset')\n",
    "print(f'Size: {len(train_images)} ({100 * len(train_images) / len(satcen):.2f}%)')\n",
    "print(f'Positive observations: {np.sum(y_train==1)} ({100 * np.sum(y_train==1) / len(y_train):.2f}%)')\n",
    "print(f'Negative observations: {np.sum(y_train==0)} ({100 * np.sum(y_train==0) / len(y_train):.2f}%)')\n",
    "\n",
    "print('\\nValidation dataset')\n",
    "print(f'Size: {len(valid_images)} ({100 * len(valid_images) / len(satcen):.2f}%)')\n",
    "print(f'Positive observations: {np.sum(y_valid==1)} ({100 * np.sum(y_valid==1) / len(y_valid):.2f}%)')\n",
    "print(f'Negative observations: {np.sum(y_valid==0)} ({100 * np.sum(y_valid==0) / len(y_valid):.2f}%)')\n",
    "\n",
    "print('\\nTest dataset')\n",
    "print(f'Size: {len(test_images)} ({100 * len(test_images) / len(satcen):.2f}%)')\n",
    "print(f'Positive observations: {np.sum(y_test==1)} ({100 * np.sum(y_test==1) / len(y_test):.2f}%)')\n",
    "print(f'Negative observations: {np.sum(y_test==0)} ({100 * np.sum(y_test==0) / len(y_test):.2f}%)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectron2 Dataset\n",
    "\n",
    "### Register Datasets\n",
    "\n",
    "Register **train** and **validation** datasets based on the existing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_type can take two possible values: train and valid\n",
    "def dataset_function(ds_type):\n",
    "\n",
    "    # determine which dataset to create\n",
    "    images = []\n",
    "    if ds_type == 'train':\n",
    "        images = train_images\n",
    "    elif ds_type == 'valid':\n",
    "        images = valid_images\n",
    "    else:\n",
    "        raise f'Invalid value for ds_type: {ds_type}'\n",
    "\n",
    "    # represents the list of dicts\n",
    "    dataset = []\n",
    "\n",
    "    # loop through all images\n",
    "    for i, image in enumerate(images):\n",
    "        \n",
    "        # list of annotations for the current image\n",
    "        annotations = []\n",
    "\n",
    "        # get the bounding boxes of the image\n",
    "        bounding_boxes = get_bounding_boxes(image)\n",
    "\n",
    "        # loop through the bounding boxes\n",
    "        for bb in bounding_boxes:\n",
    "            \n",
    "            # convert coordinates to required format\n",
    "            points = np.array([[p['x'], p['y']] for p in bb])\n",
    "\n",
    "            # get rotated rectangle\n",
    "            ((cx, cy), (w, h), a) = cv2.minAreaRect(points)\n",
    "\n",
    "            # store the annotation\n",
    "            annotations.append({\n",
    "                'bbox_mode': BoxMode.XYWHA_ABS,\n",
    "                'bbox': (cx, cy, w, h, -a),\n",
    "                'category_id': 0 # skiff\n",
    "            })\n",
    "        \n",
    "        # store the image in the dataset\n",
    "        dataset.append({\n",
    "            'file_name': satcen_path/'pictures'/image,\n",
    "            'height': 256,\n",
    "            'width': 256,\n",
    "            'image_id': i,\n",
    "            'annotations': annotations\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# register training dataset\n",
    "if 'satcen_train' in DatasetCatalog.keys():\n",
    "    DatasetCatalog.remove('satcen_train')\n",
    "DatasetCatalog.register('satcen_train', lambda x='train': dataset_function(x))\n",
    "\n",
    "# register validation dataset\n",
    "if 'satcen_valid' in DatasetCatalog.keys():\n",
    "    DatasetCatalog.remove('satcen_valid')\n",
    "DatasetCatalog.register('satcen_valid', lambda x='valid': dataset_function(x))\n",
    "\n",
    "# set metadata for the datasets\n",
    "MetadataCatalog.get('satcen_train').thing_classes = ['skiff']\n",
    "MetadataCatalog.get('satcen_valid').thing_classes = ['skiff']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Examples\n",
    "\n",
    "Show examples of the images from the original dataset and from the Detectron2 dataset to ensure the entries are formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the detectron2 dataset\n",
    "train_dataset = dataset_function('train')\n",
    "train_medatadata = MetadataCatalog.get('satcen_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only images with ships from the training dataset\n",
    "train_ship_images = [train_images[i] for i in np.where(y_train==1)[0]]\n",
    "\n",
    "# list of images to be displayed\n",
    "images_to_be_displayed = [train_ship_images[110], train_ship_images[12], train_ship_images[348]]\n",
    "\n",
    "# set up sublots \n",
    "plt.subplots(len(images_to_be_displayed), 2, figsize=(8, len(images_to_be_displayed) * 4 + 2))\n",
    "\n",
    "# display each image\n",
    "for i, filename in enumerate(images_to_be_displayed):\n",
    "    \n",
    "    # get the bounding boxes from the original dataset\n",
    "    bounding_boxes = get_bounding_boxes(filename)\n",
    "    \n",
    "    # open the image\n",
    "    img = Image.open(satcen_path/'pictures'/filename)\n",
    "    \n",
    "    # draw each bounding box\n",
    "    draw = Drawer.Draw(img)\n",
    "    for bb in bounding_boxes:\n",
    "\n",
    "        # convert coordinates to required format\n",
    "        coords = [(c['x'], c['y']) for c in bb]\n",
    "\n",
    "        # draw polygon\n",
    "        draw.polygon(coords, outline=(255,0,0))\n",
    "\n",
    "    # display the image using original format\n",
    "    plt.subplot(len(images_to_be_displayed), 2, 2*i+1)\n",
    "    plt.title('Original format')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # filter the dataset (list of dicts)\n",
    "    filtered_dict = list(filter(lambda x: x['file_name'] == satcen_path/'pictures'/filename, train_dataset))\n",
    "    if len(filtered_dict) == 0: \n",
    "        raise f'Could not find corresponding file in detectron2 dataset: {filename}'\n",
    "    entry = filtered_dict[0]\n",
    "\n",
    "    # draw bounding boxes on image\n",
    "    img = cv2.imread(str(entry['file_name']))\n",
    "    visualizer = Visualizer(img[:, :, ::-1], train_medatadata, scale=2)\n",
    "    out = visualizer.draw_dataset_dict(entry)\n",
    "\n",
    "    # display the image using detectron2 format\n",
    "    plt.subplot(len(images_to_be_displayed), 2, 2*i+2)\n",
    "    plt.title('Detectron2 format')\n",
    "    plt.imshow(out.get_image())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Construct the Model\n",
    "\n",
    "### Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config\n",
    "cfg = get_cfg()\n",
    "\n",
    "# get base model\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'))\n",
    "\n",
    "# get the pre-trained weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml')\n",
    "\n",
    "# set the train and test (validation) datasets\n",
    "cfg.DATASETS.TRAIN = ('satcen_train',)\n",
    "cfg.DATASETS.TEST = ('satcen_valid',)\n",
    "\n",
    "# set the number of classes to be predicted\n",
    "# we only have 1 class (skiff)\n",
    "# background is not considered as class\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# use RRPN as the proposal generator\n",
    "cfg.MODEL.PROPOSAL_GENERATOR.NAME = 'RRPN'\n",
    "\n",
    "# ROI head\n",
    "cfg.MODEL.ROI_HEADS.NAME = 'RROIHeads'\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # number of ROIs samples from each image\n",
    "\n",
    "# Box head\n",
    "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = 'ROIAlignRotated'\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (1,1,1,1,1)\n",
    "\n",
    "# set the bbox regression weights of the RPN\n",
    "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (1,1,1,1,1)\n",
    "\n",
    "# Anchor generator\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = 'RotatedAnchorGenerator'\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90,-60,-30,0,30,60,90]]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64, 128, 256, 512]]\n",
    "\n",
    "# specify where to save the model\n",
    "cfg.OUTPUT_DIR = './models/faster_rcnn_rrpn'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Find LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use images without skiffs in training\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True\n",
    "\n",
    "# number of ROIs sampled from each image during training\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "\n",
    "# number of images per batch/iteration (actual batch size)\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.01  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 25000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "\n",
    "# build a custom mapper for the dataloader\n",
    "def mapper(dataset_dict):\n",
    "    \n",
    "    # read image\n",
    "    image = utils.read_image(dataset_dict['file_name'], format='BGR').copy()\n",
    "    \n",
    "    # convert to format (C, H, W)\n",
    "    image = torch.from_numpy(image.transpose(2,0,1))\n",
    "\n",
    "    # convert annotations to rotated instances\n",
    "    instances = utils.annotations_to_instances_rotated(dataset_dict['annotations'], image.shape[1:])\n",
    "\n",
    "    # return model input\n",
    "    return {\n",
    "        'image': image,\n",
    "        'height': dataset_dict['height'],\n",
    "        'width': dataset_dict['width'],\n",
    "        'instances': instances\n",
    "    }  \n",
    "\n",
    "cfg.DATALOADER.ASPECT_RATIO_GROUPING = False\n",
    "\n",
    "# get the training dataloader\n",
    "dataloader = build_detection_train_loader(cfg, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/11 01:57:06 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (105, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (105,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (525, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (525,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "proposal_generator.rpn_head.anchor_deltas.{bias, weight}\n",
      "proposal_generator.rpn_head.objectness_logits.{bias, weight}\n",
      "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
      "roi_heads.box_predictor.cls_score.{bias, weight}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RRPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 105, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 525, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): RotatedAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlignRotated(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0)\n",
      "        (1): ROIAlignRotated(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0)\n",
      "        (2): ROIAlignRotated(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0)\n",
      "        (3): ROIAlignRotated(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): RotatedFastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(DefaultTrainer):\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "\n",
    "# train model\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load()\n",
    "trainer.train()\n",
    "\n",
    "# # save model\n",
    "# checkpointer = DetectionCheckpointer(model, save_dir='./models/faster_rcnn_rrpn')\n",
    "# checkpointer.save('250iter_4bs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
